{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>text</th>\n",
       "      <th>subgenre</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.880</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>0.404</td>\n",
       "      <td>135.225</td>\n",
       "      <td>373512</td>\n",
       "      <td>i feel alive steady rollin the trees, are sing...</td>\n",
       "      <td>hard rock</td>\n",
       "      <td>rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.652</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>0.650</td>\n",
       "      <td>111.904</td>\n",
       "      <td>262467</td>\n",
       "      <td>poison bell biv devoe na yeah, spyderman and f...</td>\n",
       "      <td>new jack swing</td>\n",
       "      <td>r&amp;b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.378</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.819</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.68900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.405</td>\n",
       "      <td>118.593</td>\n",
       "      <td>243067</td>\n",
       "      <td>baby it's cold outside (feat. christina aguile...</td>\n",
       "      <td>neo soul</td>\n",
       "      <td>r&amp;b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.887</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.240</td>\n",
       "      <td>130.018</td>\n",
       "      <td>193160</td>\n",
       "      <td>dumb litty kard get up out of my business you ...</td>\n",
       "      <td>dance pop</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.639</td>\n",
       "      <td>6</td>\n",
       "      <td>-6.157</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.28000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.305</td>\n",
       "      <td>147.764</td>\n",
       "      <td>224720</td>\n",
       "      <td>soldier james tw hold your breath, don't look ...</td>\n",
       "      <td>urban contemporary</td>\n",
       "      <td>r&amp;b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15400</th>\n",
       "      <td>72</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.103</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3510</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>0.340</td>\n",
       "      <td>154.962</td>\n",
       "      <td>179773</td>\n",
       "      <td>some way nav yeah, nah, nah, nah, nah, nah, na...</td>\n",
       "      <td>urban contemporary</td>\n",
       "      <td>r&amp;b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15401</th>\n",
       "      <td>0</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.800</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.211</td>\n",
       "      <td>128.012</td>\n",
       "      <td>208656</td>\n",
       "      <td>rising like the sun - radio mix qulinez caught...</td>\n",
       "      <td>progressive electro house</td>\n",
       "      <td>edm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15402</th>\n",
       "      <td>49</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.603</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.224</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.06730</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.647</td>\n",
       "      <td>129.990</td>\n",
       "      <td>260240</td>\n",
       "      <td>anaconda nicki minaj my anaconda don't, my ana...</td>\n",
       "      <td>electropop</td>\n",
       "      <td>pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15403</th>\n",
       "      <td>40</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.540</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.457</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.71500</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.657</td>\n",
       "      <td>142.218</td>\n",
       "      <td>191205</td>\n",
       "      <td>bound ponderosa twins plus one bound, bound bo...</td>\n",
       "      <td>neo soul</td>\n",
       "      <td>r&amp;b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15404</th>\n",
       "      <td>36</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.666</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.920</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>0.14300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>0.810</td>\n",
       "      <td>109.536</td>\n",
       "      <td>223890</td>\n",
       "      <td>i'll do 4 u (re-recorded / remastered) father ...</td>\n",
       "      <td>new jack swing</td>\n",
       "      <td>r&amp;b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15405 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       track_popularity  danceability  energy  key  loudness  mode  \\\n",
       "0                    28         0.303   0.880    9    -4.739     1   \n",
       "1                     0         0.845   0.652    6    -7.504     0   \n",
       "2                    41         0.425   0.378    5    -5.819     0   \n",
       "3                    65         0.760   0.887    9    -1.993     1   \n",
       "4                    70         0.496   0.639    6    -6.157     1   \n",
       "...                 ...           ...     ...  ...       ...   ...   \n",
       "15400                72         0.744   0.715    0    -6.103     1   \n",
       "15401                 0         0.479   0.800   10    -5.778     0   \n",
       "15402                49         0.963   0.603    2    -6.224     1   \n",
       "15403                40         0.458   0.540    5    -6.457     0   \n",
       "15404                36         0.832   0.666    1    -4.920     0   \n",
       "\n",
       "       speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "0           0.0442       0.01170          0.009940    0.3470    0.404   \n",
       "1           0.2160       0.00432          0.007230    0.4890    0.650   \n",
       "2           0.0341       0.68900          0.000000    0.0664    0.405   \n",
       "3           0.0409       0.03700          0.000000    0.1380    0.240   \n",
       "4           0.0550       0.28000          0.000000    0.0975    0.305   \n",
       "...            ...           ...               ...       ...      ...   \n",
       "15400       0.3510       0.10100          0.000000    0.0919    0.340   \n",
       "15401       0.0878       0.00555          0.000000    0.3350    0.211   \n",
       "15402       0.1800       0.06730          0.000006    0.2140    0.647   \n",
       "15403       0.0270       0.71500          0.000428    0.1150    0.657   \n",
       "15404       0.0633       0.14300          0.000000    0.0720    0.810   \n",
       "\n",
       "         tempo  duration_ms  \\\n",
       "0      135.225       373512   \n",
       "1      111.904       262467   \n",
       "2      118.593       243067   \n",
       "3      130.018       193160   \n",
       "4      147.764       224720   \n",
       "...        ...          ...   \n",
       "15400  154.962       179773   \n",
       "15401  128.012       208656   \n",
       "15402  129.990       260240   \n",
       "15403  142.218       191205   \n",
       "15404  109.536       223890   \n",
       "\n",
       "                                                    text  \\\n",
       "0      i feel alive steady rollin the trees, are sing...   \n",
       "1      poison bell biv devoe na yeah, spyderman and f...   \n",
       "2      baby it's cold outside (feat. christina aguile...   \n",
       "3      dumb litty kard get up out of my business you ...   \n",
       "4      soldier james tw hold your breath, don't look ...   \n",
       "...                                                  ...   \n",
       "15400  some way nav yeah, nah, nah, nah, nah, nah, na...   \n",
       "15401  rising like the sun - radio mix qulinez caught...   \n",
       "15402  anaconda nicki minaj my anaconda don't, my ana...   \n",
       "15403  bound ponderosa twins plus one bound, bound bo...   \n",
       "15404  i'll do 4 u (re-recorded / remastered) father ...   \n",
       "\n",
       "                        subgenre genre  \n",
       "0                      hard rock  rock  \n",
       "1                 new jack swing   r&b  \n",
       "2                       neo soul   r&b  \n",
       "3                      dance pop   pop  \n",
       "4             urban contemporary   r&b  \n",
       "...                          ...   ...  \n",
       "15400         urban contemporary   r&b  \n",
       "15401  progressive electro house   edm  \n",
       "15402                 electropop   pop  \n",
       "15403                   neo soul   r&b  \n",
       "15404             new jack swing   r&b  \n",
       "\n",
       "[15405 rows x 16 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('spotify_songs_cleaned.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Бейзлайн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.47      0.37      0.41       372\n",
      "       latin       0.00      0.00      0.00       161\n",
      "         pop       0.42      0.52      0.47       757\n",
      "         r&b       0.49      0.46      0.47       604\n",
      "         rap       0.57      0.57      0.57       501\n",
      "        rock       0.59      0.67      0.63       686\n",
      "\n",
      "    accuracy                           0.51      3081\n",
      "   macro avg       0.42      0.43      0.43      3081\n",
      "weighted avg       0.48      0.51      0.49      3081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "#Logreg\n",
    "X = data.select_dtypes(include=[np.number])\n",
    "y = data['genre']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.47      0.35      0.40       372\n",
      "       latin       0.21      0.14      0.17       161\n",
      "         pop       0.46      0.47      0.46       757\n",
      "         r&b       0.50      0.50      0.50       604\n",
      "         rap       0.70      0.68      0.69       501\n",
      "        rock       0.58      0.68      0.63       686\n",
      "\n",
      "    accuracy                           0.53      3081\n",
      "   macro avg       0.48      0.47      0.48      3081\n",
      "weighted avg       0.52      0.53      0.52      3081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = cv.fit_transform(X_train)\n",
    "X_test = cv.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.56      0.47      0.51       372\n",
      "       latin       0.33      0.04      0.07       161\n",
      "         pop       0.50      0.59      0.54       757\n",
      "         r&b       0.60      0.60      0.60       604\n",
      "         rap       0.76      0.75      0.76       501\n",
      "        rock       0.67      0.74      0.70       686\n",
      "\n",
      "    accuracy                           0.61      3081\n",
      "   macro avg       0.57      0.53      0.53      3081\n",
      "weighted avg       0.60      0.61      0.60      3081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Text + numerical\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "X = data.copy()\n",
    "X.drop(columns=['genre', 'subgenre'], inplace=True)\n",
    "\n",
    "y = data['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "\n",
    "X_train_text = cv.fit_transform(X_train['text'])\n",
    "X_test_text = cv.transform(X_test['text'])\n",
    "\n",
    "X_train.drop(columns='text', inplace=True)\n",
    "X_test.drop(columns='text', inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "\n",
    "X_train = hstack([X_train_text, X_train_num])\n",
    "X_test = hstack([X_test_text, X_test_num])\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre\n",
      "pop      2982\n",
      "rock     2702\n",
      "r&b      2557\n",
      "rap      2001\n",
      "edm      1386\n",
      "latin     696\n",
      "Name: count, dtype: int64\n",
      "genre\n",
      "pop      757\n",
      "rock     686\n",
      "r&b      604\n",
      "rap      501\n",
      "edm      372\n",
      "latin    161\n",
      "Name: count, dtype: int64\n",
      "genre\n",
      "pop      3739\n",
      "rock     3388\n",
      "r&b      3161\n",
      "rap      2502\n",
      "edm      1758\n",
      "latin     857\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Посмотрим на баланс классов в обучающей и тестовой выборке\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(y_test.value_counts())\n",
    "\n",
    "print(data['genre'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "rock     3739\n",
       "r&b      3739\n",
       "pop      3739\n",
       "edm      3739\n",
       "rap      3739\n",
       "latin    3739\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Сбалансируем классы в данных с помощью oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "\n",
    "\n",
    "data0 = data.copy()\n",
    "\n",
    "X = data0.copy()\n",
    "X.drop(columns=['genre', 'subgenre'], inplace=True)\n",
    "\n",
    "y = data0['genre']\n",
    "z = data0['subgenre']\n",
    "\n",
    "X, y = ros.fit_resample(X, y)\n",
    "\n",
    "data0 = pd.concat([X, y, z], axis=1)\n",
    "\n",
    "data0['genre'].value_counts()\n",
    "\n",
    "#data = data0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def get_text_length(x):\n",
    "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
    "\n",
    "def get_word_count(x):\n",
    "    return np.array([len(word_tokenize(t)) for t in x]).reshape(-1, 1)\n",
    "\n",
    "def get_unique_word_count(x):\n",
    "    return np.array([len(set(word_tokenize(t))) for t in x]).reshape(-1, 1)\n",
    "\n",
    "def get_sentiment(x):\n",
    "    return np.array([TextBlob(t).sentiment.polarity for t in x]).reshape(-1, 1)\n",
    "\n",
    "def mean_word_length(x):\n",
    "    return np.array([np.mean([len(w) for w in word_tokenize(t)]) for t in x]).reshape(-1, 1)\n",
    "\n",
    "def mean_syllable_count(x):\n",
    "    return np.array([np.mean([len(list(filter(lambda x: x.lower() in 'aeiouy', w))) for w in word_tokenize(t)]) for t in x]).reshape(-1, 1)\n",
    "\n",
    "def get_sentences_count(x):\n",
    "    return np.array([len(x.split('.')) for x in x]).reshape(-1, 1)\n",
    "\n",
    "def vocabulary_size(x):\n",
    "    return np.array([len(set(word_tokenize(x))) for x in x]).reshape(-1, 1)\n",
    "\n",
    "def sentence_similarity(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "    return similarity_matrix.mean()\n",
    "\n",
    "data['text_length'] = get_text_length(data['text'])\n",
    "data['word_count'] = get_word_count(data['text'])\n",
    "data['unique_word_count'] = get_unique_word_count(data['text'])\n",
    "data['sentiment'] = get_sentiment(data['text'])\n",
    "data['mean_word_length'] = mean_word_length(data['text'])\n",
    "data['mean_syllable_count'] = mean_syllable_count(data['text'])\n",
    "data['sentences_count'] = get_sentences_count(data['text'])\n",
    "data['vocabulary_size'] = vocabulary_size(data['text'])\n",
    "data['sentence_similarity'] = data['text'].apply(sentence_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_syllable_count</th>\n",
       "      <th>sentences_count</th>\n",
       "      <th>vocabulary_size</th>\n",
       "      <th>sentence_similarity</th>\n",
       "      <th>cluster_0</th>\n",
       "      <th>cluster_1</th>\n",
       "      <th>cluster_2</th>\n",
       "      <th>cluster_3</th>\n",
       "      <th>cluster_4</th>\n",
       "      <th>cluster_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.880</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.739</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.01170</td>\n",
       "      <td>0.009940</td>\n",
       "      <td>0.3470</td>\n",
       "      <td>...</td>\n",
       "      <td>1.364341</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>0.692730</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.652</td>\n",
       "      <td>6</td>\n",
       "      <td>-7.504</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>...</td>\n",
       "      <td>1.123003</td>\n",
       "      <td>25</td>\n",
       "      <td>206</td>\n",
       "      <td>0.288103</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.378</td>\n",
       "      <td>5</td>\n",
       "      <td>-5.819</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.68900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>...</td>\n",
       "      <td>1.476543</td>\n",
       "      <td>2</td>\n",
       "      <td>184</td>\n",
       "      <td>0.280520</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.887</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.993</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.639</td>\n",
       "      <td>6</td>\n",
       "      <td>-6.157</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>0.28000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396226</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15400</th>\n",
       "      <td>72</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.715</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.103</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3510</td>\n",
       "      <td>0.10100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>...</td>\n",
       "      <td>1.215328</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15401</th>\n",
       "      <td>0</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.800</td>\n",
       "      <td>10</td>\n",
       "      <td>-5.778</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>0.00555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>...</td>\n",
       "      <td>1.565972</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15402</th>\n",
       "      <td>49</td>\n",
       "      <td>0.963</td>\n",
       "      <td>0.603</td>\n",
       "      <td>2</td>\n",
       "      <td>-6.224</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1800</td>\n",
       "      <td>0.06730</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>...</td>\n",
       "      <td>1.244764</td>\n",
       "      <td>4</td>\n",
       "      <td>196</td>\n",
       "      <td>0.196040</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15403</th>\n",
       "      <td>40</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.540</td>\n",
       "      <td>5</td>\n",
       "      <td>-6.457</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>0.71500</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.699346</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15404</th>\n",
       "      <td>36</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.666</td>\n",
       "      <td>1</td>\n",
       "      <td>-4.920</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0633</td>\n",
       "      <td>0.14300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>...</td>\n",
       "      <td>1.281209</td>\n",
       "      <td>37</td>\n",
       "      <td>199</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15405 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       track_popularity  danceability  energy  key  loudness  mode  \\\n",
       "0                    28         0.303   0.880    9    -4.739     1   \n",
       "1                     0         0.845   0.652    6    -7.504     0   \n",
       "2                    41         0.425   0.378    5    -5.819     0   \n",
       "3                    65         0.760   0.887    9    -1.993     1   \n",
       "4                    70         0.496   0.639    6    -6.157     1   \n",
       "...                 ...           ...     ...  ...       ...   ...   \n",
       "15400                72         0.744   0.715    0    -6.103     1   \n",
       "15401                 0         0.479   0.800   10    -5.778     0   \n",
       "15402                49         0.963   0.603    2    -6.224     1   \n",
       "15403                40         0.458   0.540    5    -6.457     0   \n",
       "15404                36         0.832   0.666    1    -4.920     0   \n",
       "\n",
       "       speechiness  acousticness  instrumentalness  liveness  ...  \\\n",
       "0           0.0442       0.01170          0.009940    0.3470  ...   \n",
       "1           0.2160       0.00432          0.007230    0.4890  ...   \n",
       "2           0.0341       0.68900          0.000000    0.0664  ...   \n",
       "3           0.0409       0.03700          0.000000    0.1380  ...   \n",
       "4           0.0550       0.28000          0.000000    0.0975  ...   \n",
       "...            ...           ...               ...       ...  ...   \n",
       "15400       0.3510       0.10100          0.000000    0.0919  ...   \n",
       "15401       0.0878       0.00555          0.000000    0.3350  ...   \n",
       "15402       0.1800       0.06730          0.000006    0.2140  ...   \n",
       "15403       0.0270       0.71500          0.000428    0.1150  ...   \n",
       "15404       0.0633       0.14300          0.000000    0.0720  ...   \n",
       "\n",
       "       mean_syllable_count  sentences_count  vocabulary_size  \\\n",
       "0                 1.364341                1               67   \n",
       "1                 1.123003               25              206   \n",
       "2                 1.476543                2              184   \n",
       "3                 0.852459                1              163   \n",
       "4                 1.396226                1              140   \n",
       "...                    ...              ...              ...   \n",
       "15400             1.215328                1              171   \n",
       "15401             1.565972                1               82   \n",
       "15402             1.244764                4              196   \n",
       "15403             1.699346                1               58   \n",
       "15404             1.281209               37              199   \n",
       "\n",
       "      sentence_similarity cluster_0 cluster_1  cluster_2  cluster_3  \\\n",
       "0                0.692730     False     False       True      False   \n",
       "1                0.288103     False     False      False       True   \n",
       "2                0.280520     False     False      False      False   \n",
       "3                1.000000     False      True      False      False   \n",
       "4                1.000000     False     False      False      False   \n",
       "...                   ...       ...       ...        ...        ...   \n",
       "15400            1.000000     False      True      False      False   \n",
       "15401            1.000000     False      True      False      False   \n",
       "15402            0.196040     False     False      False       True   \n",
       "15403            1.000000     False      True      False      False   \n",
       "15404            1.000000     False     False      False      False   \n",
       "\n",
       "       cluster_4  cluster_5  \n",
       "0          False      False  \n",
       "1          False      False  \n",
       "2           True      False  \n",
       "3          False      False  \n",
       "4           True      False  \n",
       "...          ...        ...  \n",
       "15400      False      False  \n",
       "15401      False      False  \n",
       "15402      False      False  \n",
       "15403      False      False  \n",
       "15404       True      False  \n",
       "\n",
       "[15405 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Кластеринг \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=6)\n",
    "\n",
    "X = data.select_dtypes(include=[np.number])\n",
    "\n",
    "data['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "#OneHotEncoding для кластеров\n",
    "data = pd.get_dummies(data, columns=['cluster'])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.58      0.53      0.55       352\n",
      "       latin       0.50      0.05      0.10       171\n",
      "         pop       0.53      0.61      0.57       748\n",
      "         r&b       0.64      0.63      0.63       632\n",
      "         rap       0.77      0.77      0.77       500\n",
      "        rock       0.70      0.77      0.73       678\n",
      "\n",
      "    accuracy                           0.63      3081\n",
      "   macro avg       0.62      0.56      0.56      3081\n",
      "weighted avg       0.63      0.63      0.62      3081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Feature engineering + стратификация\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "X = data.copy()\n",
    "X.drop(columns=['genre', 'subgenre'], inplace=True)\n",
    "\n",
    "y = data['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "\n",
    "X_train_text = cv.fit_transform(X_train['text'])\n",
    "X_test_text = cv.transform(X_test['text'])\n",
    "\n",
    "X_train.drop(columns='text', inplace=True)\n",
    "X_test.drop(columns='text', inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "\n",
    "X_train = hstack([X_train_text, X_train_num])\n",
    "X_test = hstack([X_test_text, X_test_num])\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.53      0.52      0.52       352\n",
      "       latin       0.12      0.13      0.13       171\n",
      "         pop       0.43      0.45      0.44       748\n",
      "         r&b       0.49      0.47      0.48       632\n",
      "         rap       0.64      0.63      0.63       500\n",
      "        rock       0.61      0.61      0.61       678\n",
      "\n",
      "    accuracy                           0.51      3081\n",
      "   macro avg       0.47      0.47      0.47      3081\n",
      "weighted avg       0.51      0.51      0.51      3081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "X = data.copy()\n",
    "X.drop(columns=['genre', 'subgenre'], inplace=True)\n",
    "\n",
    "y = data['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "\n",
    "X_train_text = cv.fit_transform(X_train['text'])\n",
    "X_test_text = cv.transform(X_test['text'])\n",
    "\n",
    "X_train.drop(columns='text', inplace=True)\n",
    "X_test.drop(columns='text', inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "\n",
    "X_train = hstack([X_train_text, X_train_num])\n",
    "X_test = hstack([X_test_text, X_test_num])\n",
    "\n",
    "#Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.63      0.40      0.49       352\n",
      "       latin       0.24      0.06      0.10       171\n",
      "         pop       0.46      0.63      0.53       748\n",
      "         r&b       0.65      0.49      0.56       632\n",
      "         rap       0.72      0.76      0.74       500\n",
      "        rock       0.65      0.76      0.70       678\n",
      "\n",
      "    accuracy                           0.59      3081\n",
      "   macro avg       0.56      0.52      0.52      3081\n",
      "weighted avg       0.59      0.59      0.58      3081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.38      0.46      0.42       352\n",
      "       latin       0.08      0.05      0.06       171\n",
      "         pop       0.41      0.53      0.47       748\n",
      "         r&b       0.55      0.50      0.52       632\n",
      "         rap       0.73      0.62      0.67       500\n",
      "        rock       0.60      0.52      0.56       678\n",
      "\n",
      "    accuracy                           0.50      3081\n",
      "   macro avg       0.46      0.45      0.45      3081\n",
      "weighted avg       0.51      0.50      0.50      3081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Knn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.72      0.63      0.67       352\n",
      "       latin       0.64      0.12      0.21       171\n",
      "         pop       0.53      0.63      0.58       748\n",
      "         r&b       0.68      0.67      0.67       632\n",
      "         rap       0.76      0.78      0.77       500\n",
      "        rock       0.74      0.78      0.76       678\n",
      "\n",
      "    accuracy                           0.67      3081\n",
      "   macro avg       0.68      0.60      0.61      3081\n",
      "weighted avg       0.67      0.67      0.66      3081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gradient boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пробуем по разному векторизовать текст\n",
    "data1 = data.copy()\n",
    "\n",
    "#Токенизируем текст\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data1['text'] = data1['text'].apply(word_tokenize)\n",
    "\n",
    "#Оставим только слова (удалим пунктуацию и символы)\n",
    "import re\n",
    "\n",
    "data1['text'] = data1['text'].apply(lambda x: [word for word in x if re.match(r'\\w+', word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport nltk\\n\\nnltk.download('stopwords')\\nnltk.download('punkt')\\n\\nfrom nltk.corpus import stopwords\\n\\nstop_words = list(stopwords.words('english'))\\n\\nimport string\\n\\nnoise = set(stop_words + list(string.punctuation))\\n\\ndef remove_noise(text):\\n    return [word.lower() for word in text if word.lower() not in noise]\\n\\ndata1['text'] = data1['text'].apply(remove_noise)\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "import string\n",
    "\n",
    "noise = set(stop_words + list(string.punctuation))\n",
    "\n",
    "def remove_noise(text):\n",
    "    return [word.lower() for word in text if word.lower() not in noise]\n",
    "\n",
    "data1['text'] = data1['text'].apply(remove_noise)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15405/15405 [01:18<00:00, 195.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#Лемматизация pymorphy3\n",
    "import pymorphy3\n",
    "import tqdm\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return [morph.parse(word)[0].normal_form for word in text]\n",
    "\n",
    "#Отслеживаем прогресс \n",
    "tqdm.tqdm.pandas()\n",
    "\n",
    "data1['text'] = data1['text'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.59      0.53      0.56       352\n",
      "       latin       0.45      0.05      0.09       171\n",
      "         pop       0.53      0.61      0.57       748\n",
      "         r&b       0.63      0.62      0.63       632\n",
      "         rap       0.77      0.76      0.76       500\n",
      "        rock       0.69      0.77      0.73       678\n",
      "\n",
      "    accuracy                           0.63      3081\n",
      "   macro avg       0.61      0.56      0.56      3081\n",
      "weighted avg       0.63      0.63      0.62      3081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "X = data1.copy()\n",
    "X.drop(columns=['genre', 'subgenre'], inplace=True)\n",
    "\n",
    "y = data1['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(' '.join)\n",
    "X_test['text'] = X_test['text'].apply(' '.join)\n",
    "\n",
    "X_train_text = cv.fit_transform(X_train['text'])\n",
    "X_test_text = cv.transform(X_test['text'])\n",
    "\n",
    "X_train.drop(columns='text', inplace=True)\n",
    "X_test.drop(columns='text', inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "\n",
    "X_train = hstack([X_train_text, X_train_num])\n",
    "X_test = hstack([X_test_text, X_test_num])\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.58      0.49      0.53       352\n",
      "       latin       0.32      0.04      0.06       171\n",
      "         pop       0.51      0.57      0.54       748\n",
      "         r&b       0.59      0.62      0.61       632\n",
      "         rap       0.75      0.74      0.75       500\n",
      "        rock       0.66      0.73      0.69       678\n",
      "\n",
      "    accuracy                           0.61      3081\n",
      "   macro avg       0.57      0.53      0.53      3081\n",
      "weighted avg       0.59      0.61      0.59      3081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#word2vec векторизация\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     vector_size=100,\n",
    "                     sample=6e-5,\n",
    "                     alpha=0.03,\n",
    "                     min_alpha=0.0007,\n",
    "                     negative=20,\n",
    "                     workers=2)\n",
    "\n",
    "X = data1.copy()\n",
    "X.drop(columns=['genre', 'subgenre'], inplace=True)\n",
    "\n",
    "y = data1['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "model.build_vocab(X_train['text'])\n",
    "model.train(X_train['text'], total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "def text_to_vec(text):\n",
    "    vec = np.zeros(100)\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vec / count\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(text_to_vec)\n",
    "X_test['text'] = X_test['text'].apply(text_to_vec)\n",
    "\n",
    "X_train_text = np.stack(X_train['text'])\n",
    "X_test_text = np.stack(X_test['text'])\n",
    "\n",
    "X_train.drop(columns='text', inplace=True)\n",
    "X_test.drop(columns='text', inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "\n",
    "X_train = np.hstack([X_train_text, X_train_num])\n",
    "X_test = np.hstack([X_test_text, X_test_num])\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.57      0.49      0.53       352\n",
      "       latin       0.44      0.05      0.08       171\n",
      "         pop       0.51      0.58      0.54       748\n",
      "         r&b       0.60      0.62      0.61       632\n",
      "         rap       0.75      0.75      0.75       500\n",
      "        rock       0.65      0.72      0.68       678\n",
      "\n",
      "    accuracy                           0.61      3081\n",
      "   macro avg       0.59      0.54      0.53      3081\n",
      "weighted avg       0.60      0.61      0.59      3081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#fasttext\n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "model = FastText()\n",
    "\n",
    "X = data1.copy()\n",
    "X.drop(columns=['genre', 'subgenre'], inplace=True)\n",
    "\n",
    "y = data1['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "model.build_vocab(X_train['text'])\n",
    "model.train(X_train['text'], total_examples=model.corpus_count, epochs=30)\n",
    "\n",
    "def text_to_vec(text):\n",
    "    vec = np.zeros(100)\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += model.wv[word]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vec / count\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(text_to_vec)\n",
    "X_test['text'] = X_test['text'].apply(text_to_vec)\n",
    "\n",
    "X_train_text = np.stack(X_train['text'])\n",
    "X_test_text = np.stack(X_test['text'])\n",
    "\n",
    "X_train.drop(columns='text', inplace=True)\n",
    "X_test.drop(columns='text', inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "\n",
    "X_train = np.hstack([X_train_text, X_train_num])\n",
    "\n",
    "X_test = np.hstack([X_test_text, X_test_num])\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.56      0.51      0.53       352\n",
      "       latin       0.57      0.05      0.09       171\n",
      "         pop       0.51      0.57      0.54       748\n",
      "         r&b       0.58      0.62      0.60       632\n",
      "         rap       0.76      0.75      0.75       500\n",
      "        rock       0.65      0.72      0.68       678\n",
      "\n",
      "    accuracy                           0.61      3081\n",
      "   macro avg       0.61      0.54      0.53      3081\n",
      "weighted avg       0.61      0.61      0.59      3081\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\GitHub\\ml_music_classification\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Попробуем предобученный векторайзер\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "X = data1.copy()\n",
    "X.drop(columns=['genre', 'subgenre'], inplace=True)\n",
    "\n",
    "y = data1['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "def text_to_vec(text, model = glove_vectors):\n",
    "    vec = np.zeros(100)\n",
    "    count = 0\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += model[word]\n",
    "            count += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return vec / count\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(text_to_vec)\n",
    "X_test['text'] = X_test['text'].apply(text_to_vec)\n",
    "\n",
    "X_train_text = np.stack(X_train['text'])\n",
    "X_test_text = np.stack(X_test['text'])\n",
    "\n",
    "X_train.drop(columns='text', inplace=True)\n",
    "X_test.drop(columns='text', inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "\n",
    "X_train = np.hstack([X_train_text, X_train_num])\n",
    "X_test = np.hstack([X_test_text, X_test_num])\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.65      0.50      0.57       352\n",
      "       latin       0.75      0.02      0.03       171\n",
      "         pop       0.51      0.64      0.57       748\n",
      "         r&b       0.62      0.60      0.61       632\n",
      "         rap       0.75      0.75      0.75       500\n",
      "        rock       0.67      0.75      0.71       678\n",
      "\n",
      "    accuracy                           0.62      3081\n",
      "   macro avg       0.66      0.54      0.54      3081\n",
      "weighted avg       0.64      0.62      0.61      3081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SVM \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.09168\n",
      "0:\tlearn: 1.7269887\ttotal: 1.52s\tremaining: 25m 21s\n",
      "1:\tlearn: 1.6647722\ttotal: 3.24s\tremaining: 26m 56s\n",
      "2:\tlearn: 1.6217227\ttotal: 4.92s\tremaining: 27m 15s\n",
      "3:\tlearn: 1.5818443\ttotal: 6.51s\tremaining: 27m\n",
      "4:\tlearn: 1.5448640\ttotal: 8.05s\tremaining: 26m 43s\n",
      "5:\tlearn: 1.5205856\ttotal: 9.38s\tremaining: 25m 53s\n",
      "6:\tlearn: 1.4926663\ttotal: 10.7s\tremaining: 25m 18s\n",
      "7:\tlearn: 1.4700965\ttotal: 12s\tremaining: 24m 53s\n",
      "8:\tlearn: 1.4477021\ttotal: 13.5s\tremaining: 24m 47s\n",
      "9:\tlearn: 1.4301201\ttotal: 14.8s\tremaining: 24m 27s\n",
      "10:\tlearn: 1.4095445\ttotal: 16.1s\tremaining: 24m 10s\n",
      "11:\tlearn: 1.3932407\ttotal: 17.6s\tremaining: 24m 9s\n",
      "12:\tlearn: 1.3761242\ttotal: 19s\tremaining: 23m 59s\n",
      "13:\tlearn: 1.3615284\ttotal: 20.3s\tremaining: 23m 48s\n",
      "14:\tlearn: 1.3495344\ttotal: 21.6s\tremaining: 23m 38s\n",
      "15:\tlearn: 1.3342508\ttotal: 23.1s\tremaining: 23m 40s\n",
      "16:\tlearn: 1.3252867\ttotal: 24.4s\tremaining: 23m 29s\n",
      "17:\tlearn: 1.3116889\ttotal: 25.9s\tremaining: 23m 30s\n",
      "18:\tlearn: 1.2995132\ttotal: 27.3s\tremaining: 23m 29s\n",
      "19:\tlearn: 1.2889474\ttotal: 28.6s\tremaining: 23m 21s\n",
      "20:\tlearn: 1.2802426\ttotal: 30.1s\tremaining: 23m 21s\n",
      "21:\tlearn: 1.2696996\ttotal: 31.5s\tremaining: 23m 18s\n",
      "22:\tlearn: 1.2622188\ttotal: 32.8s\tremaining: 23m 12s\n",
      "23:\tlearn: 1.2576459\ttotal: 34.1s\tremaining: 23m 6s\n",
      "24:\tlearn: 1.2493960\ttotal: 35.6s\tremaining: 23m 7s\n",
      "25:\tlearn: 1.2449628\ttotal: 36.9s\tremaining: 23m 1s\n",
      "26:\tlearn: 1.2369837\ttotal: 38.4s\tremaining: 23m 2s\n",
      "27:\tlearn: 1.2296174\ttotal: 39.7s\tremaining: 22m 57s\n",
      "28:\tlearn: 1.2225646\ttotal: 41s\tremaining: 22m 54s\n",
      "29:\tlearn: 1.2167089\ttotal: 42.4s\tremaining: 22m 49s\n",
      "30:\tlearn: 1.2082162\ttotal: 43.8s\tremaining: 22m 50s\n",
      "31:\tlearn: 1.2024108\ttotal: 45.3s\tremaining: 22m 50s\n",
      "32:\tlearn: 1.1963628\ttotal: 46.6s\tremaining: 22m 46s\n",
      "33:\tlearn: 1.1930611\ttotal: 48s\tremaining: 22m 44s\n",
      "34:\tlearn: 1.1863694\ttotal: 49.3s\tremaining: 22m 40s\n",
      "35:\tlearn: 1.1815053\ttotal: 50.6s\tremaining: 22m 36s\n",
      "36:\tlearn: 1.1783419\ttotal: 52.1s\tremaining: 22m 35s\n",
      "37:\tlearn: 1.1732191\ttotal: 53.4s\tremaining: 22m 31s\n",
      "38:\tlearn: 1.1685262\ttotal: 54.7s\tremaining: 22m 27s\n",
      "39:\tlearn: 1.1644364\ttotal: 56s\tremaining: 22m 23s\n",
      "40:\tlearn: 1.1602931\ttotal: 57.4s\tremaining: 22m 21s\n",
      "41:\tlearn: 1.1574031\ttotal: 58.6s\tremaining: 22m 17s\n",
      "42:\tlearn: 1.1533888\ttotal: 60s\tremaining: 22m 14s\n",
      "43:\tlearn: 1.1510373\ttotal: 1m 1s\tremaining: 22m 10s\n",
      "44:\tlearn: 1.1474921\ttotal: 1m 2s\tremaining: 22m 7s\n",
      "45:\tlearn: 1.1424181\ttotal: 1m 3s\tremaining: 22m 4s\n",
      "46:\tlearn: 1.1400335\ttotal: 1m 5s\tremaining: 22m 1s\n",
      "47:\tlearn: 1.1386363\ttotal: 1m 6s\tremaining: 21m 58s\n",
      "48:\tlearn: 1.1344516\ttotal: 1m 7s\tremaining: 21m 55s\n",
      "49:\tlearn: 1.1309641\ttotal: 1m 9s\tremaining: 21m 53s\n",
      "50:\tlearn: 1.1273775\ttotal: 1m 10s\tremaining: 21m 51s\n",
      "51:\tlearn: 1.1238239\ttotal: 1m 11s\tremaining: 21m 48s\n",
      "52:\tlearn: 1.1214020\ttotal: 1m 13s\tremaining: 21m 46s\n",
      "53:\tlearn: 1.1198587\ttotal: 1m 14s\tremaining: 21m 43s\n",
      "54:\tlearn: 1.1168668\ttotal: 1m 15s\tremaining: 21m 41s\n",
      "55:\tlearn: 1.1148730\ttotal: 1m 17s\tremaining: 21m 38s\n",
      "56:\tlearn: 1.1124357\ttotal: 1m 18s\tremaining: 21m 36s\n",
      "57:\tlearn: 1.1100052\ttotal: 1m 19s\tremaining: 21m 33s\n",
      "58:\tlearn: 1.1090784\ttotal: 1m 20s\tremaining: 21m 31s\n",
      "59:\tlearn: 1.1060584\ttotal: 1m 22s\tremaining: 21m 28s\n",
      "60:\tlearn: 1.1046583\ttotal: 1m 23s\tremaining: 21m 26s\n",
      "61:\tlearn: 1.1037040\ttotal: 1m 24s\tremaining: 21m 23s\n",
      "62:\tlearn: 1.1017618\ttotal: 1m 26s\tremaining: 21m 22s\n",
      "63:\tlearn: 1.1004223\ttotal: 1m 27s\tremaining: 21m 20s\n",
      "64:\tlearn: 1.0981967\ttotal: 1m 28s\tremaining: 21m 17s\n",
      "65:\tlearn: 1.0943154\ttotal: 1m 30s\tremaining: 21m 15s\n",
      "66:\tlearn: 1.0917330\ttotal: 1m 31s\tremaining: 21m 13s\n",
      "67:\tlearn: 1.0911844\ttotal: 1m 32s\tremaining: 21m 10s\n",
      "68:\tlearn: 1.0898602\ttotal: 1m 34s\tremaining: 21m 8s\n",
      "69:\tlearn: 1.0876654\ttotal: 1m 35s\tremaining: 21m 6s\n",
      "70:\tlearn: 1.0860235\ttotal: 1m 36s\tremaining: 21m 4s\n",
      "71:\tlearn: 1.0843760\ttotal: 1m 37s\tremaining: 21m 2s\n",
      "72:\tlearn: 1.0833127\ttotal: 1m 39s\tremaining: 21m\n",
      "73:\tlearn: 1.0797338\ttotal: 1m 40s\tremaining: 20m 58s\n",
      "74:\tlearn: 1.0780267\ttotal: 1m 41s\tremaining: 20m 56s\n",
      "75:\tlearn: 1.0757468\ttotal: 1m 43s\tremaining: 20m 54s\n",
      "76:\tlearn: 1.0741033\ttotal: 1m 44s\tremaining: 20m 52s\n",
      "77:\tlearn: 1.0734708\ttotal: 1m 45s\tremaining: 20m 50s\n",
      "78:\tlearn: 1.0723684\ttotal: 1m 47s\tremaining: 20m 48s\n",
      "79:\tlearn: 1.0700357\ttotal: 1m 48s\tremaining: 20m 46s\n",
      "80:\tlearn: 1.0680087\ttotal: 1m 49s\tremaining: 20m 45s\n",
      "81:\tlearn: 1.0660057\ttotal: 1m 51s\tremaining: 20m 43s\n",
      "82:\tlearn: 1.0652269\ttotal: 1m 52s\tremaining: 20m 41s\n",
      "83:\tlearn: 1.0630253\ttotal: 1m 53s\tremaining: 20m 39s\n",
      "84:\tlearn: 1.0616699\ttotal: 1m 54s\tremaining: 20m 37s\n",
      "85:\tlearn: 1.0588094\ttotal: 1m 56s\tremaining: 20m 37s\n",
      "86:\tlearn: 1.0576688\ttotal: 1m 57s\tremaining: 20m 36s\n",
      "87:\tlearn: 1.0565262\ttotal: 1m 59s\tremaining: 20m 34s\n",
      "88:\tlearn: 1.0536397\ttotal: 2m\tremaining: 20m 32s\n",
      "89:\tlearn: 1.0520877\ttotal: 2m 1s\tremaining: 20m 30s\n",
      "90:\tlearn: 1.0511918\ttotal: 2m 3s\tremaining: 20m 28s\n",
      "91:\tlearn: 1.0504647\ttotal: 2m 4s\tremaining: 20m 27s\n",
      "92:\tlearn: 1.0493122\ttotal: 2m 5s\tremaining: 20m 25s\n",
      "93:\tlearn: 1.0481081\ttotal: 2m 7s\tremaining: 20m 24s\n",
      "94:\tlearn: 1.0470674\ttotal: 2m 8s\tremaining: 20m 23s\n",
      "95:\tlearn: 1.0439716\ttotal: 2m 9s\tremaining: 20m 22s\n",
      "96:\tlearn: 1.0421579\ttotal: 2m 11s\tremaining: 20m 21s\n",
      "97:\tlearn: 1.0403833\ttotal: 2m 12s\tremaining: 20m 20s\n",
      "98:\tlearn: 1.0365145\ttotal: 2m 14s\tremaining: 20m 20s\n",
      "99:\tlearn: 1.0340572\ttotal: 2m 15s\tremaining: 20m 20s\n",
      "100:\tlearn: 1.0325886\ttotal: 2m 16s\tremaining: 20m 18s\n",
      "101:\tlearn: 1.0314550\ttotal: 2m 18s\tremaining: 20m 16s\n",
      "102:\tlearn: 1.0305369\ttotal: 2m 19s\tremaining: 20m 14s\n",
      "103:\tlearn: 1.0296034\ttotal: 2m 20s\tremaining: 20m 12s\n",
      "104:\tlearn: 1.0288688\ttotal: 2m 22s\tremaining: 20m 10s\n",
      "105:\tlearn: 1.0271743\ttotal: 2m 23s\tremaining: 20m 9s\n",
      "106:\tlearn: 1.0260613\ttotal: 2m 24s\tremaining: 20m 7s\n",
      "107:\tlearn: 1.0243521\ttotal: 2m 26s\tremaining: 20m 6s\n",
      "108:\tlearn: 1.0213897\ttotal: 2m 27s\tremaining: 20m 4s\n",
      "109:\tlearn: 1.0204231\ttotal: 2m 28s\tremaining: 20m 2s\n",
      "110:\tlearn: 1.0188633\ttotal: 2m 29s\tremaining: 20m 1s\n",
      "111:\tlearn: 1.0171369\ttotal: 2m 31s\tremaining: 19m 59s\n",
      "112:\tlearn: 1.0155145\ttotal: 2m 32s\tremaining: 19m 58s\n",
      "113:\tlearn: 1.0141648\ttotal: 2m 34s\tremaining: 19m 58s\n",
      "114:\tlearn: 1.0124588\ttotal: 2m 35s\tremaining: 19m 56s\n",
      "115:\tlearn: 1.0109948\ttotal: 2m 36s\tremaining: 19m 55s\n",
      "116:\tlearn: 1.0088872\ttotal: 2m 38s\tremaining: 19m 55s\n",
      "117:\tlearn: 1.0078704\ttotal: 2m 39s\tremaining: 19m 53s\n",
      "118:\tlearn: 1.0068946\ttotal: 2m 40s\tremaining: 19m 51s\n",
      "119:\tlearn: 1.0047180\ttotal: 2m 42s\tremaining: 19m 49s\n",
      "120:\tlearn: 1.0029594\ttotal: 2m 43s\tremaining: 19m 47s\n",
      "121:\tlearn: 1.0022767\ttotal: 2m 44s\tremaining: 19m 47s\n",
      "122:\tlearn: 0.9993937\ttotal: 2m 46s\tremaining: 19m 45s\n",
      "123:\tlearn: 0.9983466\ttotal: 2m 47s\tremaining: 19m 44s\n",
      "124:\tlearn: 0.9961522\ttotal: 2m 49s\tremaining: 19m 43s\n",
      "125:\tlearn: 0.9942358\ttotal: 2m 50s\tremaining: 19m 42s\n",
      "126:\tlearn: 0.9925760\ttotal: 2m 51s\tremaining: 19m 41s\n",
      "127:\tlearn: 0.9911383\ttotal: 2m 53s\tremaining: 19m 40s\n",
      "128:\tlearn: 0.9901358\ttotal: 2m 54s\tremaining: 19m 39s\n",
      "129:\tlearn: 0.9893196\ttotal: 2m 55s\tremaining: 19m 37s\n",
      "130:\tlearn: 0.9876411\ttotal: 2m 57s\tremaining: 19m 36s\n",
      "131:\tlearn: 0.9866299\ttotal: 2m 58s\tremaining: 19m 34s\n",
      "132:\tlearn: 0.9854585\ttotal: 3m\tremaining: 19m 34s\n",
      "133:\tlearn: 0.9831551\ttotal: 3m 1s\tremaining: 19m 33s\n",
      "134:\tlearn: 0.9825376\ttotal: 3m 2s\tremaining: 19m 31s\n",
      "135:\tlearn: 0.9819901\ttotal: 3m 4s\tremaining: 19m 30s\n",
      "136:\tlearn: 0.9786324\ttotal: 3m 5s\tremaining: 19m 30s\n",
      "137:\tlearn: 0.9759656\ttotal: 3m 7s\tremaining: 19m 29s\n",
      "138:\tlearn: 0.9755702\ttotal: 3m 8s\tremaining: 19m 27s\n",
      "139:\tlearn: 0.9744737\ttotal: 3m 9s\tremaining: 19m 26s\n",
      "140:\tlearn: 0.9735006\ttotal: 3m 11s\tremaining: 19m 25s\n",
      "141:\tlearn: 0.9727452\ttotal: 3m 12s\tremaining: 19m 23s\n",
      "142:\tlearn: 0.9704351\ttotal: 3m 13s\tremaining: 19m 22s\n",
      "143:\tlearn: 0.9692377\ttotal: 3m 15s\tremaining: 19m 20s\n",
      "144:\tlearn: 0.9685512\ttotal: 3m 16s\tremaining: 19m 18s\n",
      "145:\tlearn: 0.9679189\ttotal: 3m 17s\tremaining: 19m 16s\n",
      "146:\tlearn: 0.9669767\ttotal: 3m 19s\tremaining: 19m 15s\n",
      "147:\tlearn: 0.9664109\ttotal: 3m 20s\tremaining: 19m 13s\n",
      "148:\tlearn: 0.9655077\ttotal: 3m 21s\tremaining: 19m 11s\n",
      "149:\tlearn: 0.9636948\ttotal: 3m 22s\tremaining: 19m 9s\n",
      "150:\tlearn: 0.9624739\ttotal: 3m 24s\tremaining: 19m 8s\n",
      "151:\tlearn: 0.9615920\ttotal: 3m 25s\tremaining: 19m 7s\n",
      "152:\tlearn: 0.9607644\ttotal: 3m 26s\tremaining: 19m 5s\n",
      "153:\tlearn: 0.9593996\ttotal: 3m 28s\tremaining: 19m 3s\n",
      "154:\tlearn: 0.9576979\ttotal: 3m 29s\tremaining: 19m 2s\n",
      "155:\tlearn: 0.9568754\ttotal: 3m 30s\tremaining: 19m\n",
      "156:\tlearn: 0.9547280\ttotal: 3m 32s\tremaining: 18m 59s\n",
      "157:\tlearn: 0.9541436\ttotal: 3m 33s\tremaining: 18m 57s\n",
      "158:\tlearn: 0.9532319\ttotal: 3m 34s\tremaining: 18m 55s\n",
      "159:\tlearn: 0.9517949\ttotal: 3m 35s\tremaining: 18m 53s\n",
      "160:\tlearn: 0.9512430\ttotal: 3m 37s\tremaining: 18m 52s\n",
      "161:\tlearn: 0.9508553\ttotal: 3m 38s\tremaining: 18m 50s\n",
      "162:\tlearn: 0.9502280\ttotal: 3m 39s\tremaining: 18m 48s\n",
      "163:\tlearn: 0.9483484\ttotal: 3m 41s\tremaining: 18m 47s\n",
      "164:\tlearn: 0.9471870\ttotal: 3m 42s\tremaining: 18m 45s\n",
      "165:\tlearn: 0.9465655\ttotal: 3m 43s\tremaining: 18m 44s\n",
      "166:\tlearn: 0.9461219\ttotal: 3m 45s\tremaining: 18m 43s\n",
      "167:\tlearn: 0.9450718\ttotal: 3m 46s\tremaining: 18m 41s\n",
      "168:\tlearn: 0.9442709\ttotal: 3m 47s\tremaining: 18m 39s\n",
      "169:\tlearn: 0.9439994\ttotal: 3m 49s\tremaining: 18m 38s\n",
      "170:\tlearn: 0.9432445\ttotal: 3m 50s\tremaining: 18m 36s\n",
      "171:\tlearn: 0.9427726\ttotal: 3m 51s\tremaining: 18m 34s\n",
      "172:\tlearn: 0.9410551\ttotal: 3m 52s\tremaining: 18m 33s\n",
      "173:\tlearn: 0.9408128\ttotal: 3m 54s\tremaining: 18m 31s\n",
      "174:\tlearn: 0.9402799\ttotal: 3m 55s\tremaining: 18m 30s\n",
      "175:\tlearn: 0.9398594\ttotal: 3m 56s\tremaining: 18m 28s\n",
      "176:\tlearn: 0.9390147\ttotal: 3m 58s\tremaining: 18m 26s\n",
      "177:\tlearn: 0.9385478\ttotal: 3m 59s\tremaining: 18m 25s\n",
      "178:\tlearn: 0.9374591\ttotal: 4m\tremaining: 18m 24s\n",
      "179:\tlearn: 0.9369282\ttotal: 4m 2s\tremaining: 18m 22s\n",
      "180:\tlearn: 0.9357132\ttotal: 4m 3s\tremaining: 18m 21s\n",
      "181:\tlearn: 0.9351666\ttotal: 4m 4s\tremaining: 18m 19s\n",
      "182:\tlearn: 0.9347066\ttotal: 4m 5s\tremaining: 18m 17s\n",
      "183:\tlearn: 0.9337964\ttotal: 4m 7s\tremaining: 18m 17s\n",
      "184:\tlearn: 0.9333176\ttotal: 4m 8s\tremaining: 18m 15s\n",
      "185:\tlearn: 0.9327049\ttotal: 4m 9s\tremaining: 18m 13s\n",
      "186:\tlearn: 0.9314960\ttotal: 4m 11s\tremaining: 18m 12s\n",
      "187:\tlearn: 0.9305255\ttotal: 4m 12s\tremaining: 18m 10s\n",
      "188:\tlearn: 0.9297680\ttotal: 4m 13s\tremaining: 18m 9s\n",
      "189:\tlearn: 0.9293898\ttotal: 4m 15s\tremaining: 18m 7s\n",
      "190:\tlearn: 0.9287197\ttotal: 4m 16s\tremaining: 18m 5s\n",
      "191:\tlearn: 0.9284307\ttotal: 4m 17s\tremaining: 18m 4s\n",
      "192:\tlearn: 0.9277458\ttotal: 4m 18s\tremaining: 18m 2s\n",
      "193:\tlearn: 0.9267423\ttotal: 4m 20s\tremaining: 18m 1s\n",
      "194:\tlearn: 0.9263184\ttotal: 4m 21s\tremaining: 17m 59s\n",
      "195:\tlearn: 0.9251482\ttotal: 4m 22s\tremaining: 17m 58s\n",
      "196:\tlearn: 0.9246592\ttotal: 4m 24s\tremaining: 17m 56s\n",
      "197:\tlearn: 0.9235193\ttotal: 4m 25s\tremaining: 17m 55s\n",
      "198:\tlearn: 0.9226956\ttotal: 4m 26s\tremaining: 17m 54s\n",
      "199:\tlearn: 0.9218308\ttotal: 4m 28s\tremaining: 17m 52s\n",
      "200:\tlearn: 0.9205908\ttotal: 4m 29s\tremaining: 17m 51s\n",
      "201:\tlearn: 0.9201164\ttotal: 4m 30s\tremaining: 17m 49s\n",
      "202:\tlearn: 0.9198318\ttotal: 4m 32s\tremaining: 17m 48s\n",
      "203:\tlearn: 0.9186116\ttotal: 4m 33s\tremaining: 17m 46s\n",
      "204:\tlearn: 0.9182048\ttotal: 4m 34s\tremaining: 17m 45s\n",
      "205:\tlearn: 0.9176658\ttotal: 4m 36s\tremaining: 17m 43s\n",
      "206:\tlearn: 0.9172832\ttotal: 4m 37s\tremaining: 17m 42s\n",
      "207:\tlearn: 0.9168913\ttotal: 4m 38s\tremaining: 17m 40s\n",
      "208:\tlearn: 0.9158542\ttotal: 4m 39s\tremaining: 17m 39s\n",
      "209:\tlearn: 0.9154787\ttotal: 4m 41s\tremaining: 17m 37s\n",
      "210:\tlearn: 0.9150909\ttotal: 4m 42s\tremaining: 17m 36s\n",
      "211:\tlearn: 0.9144989\ttotal: 4m 43s\tremaining: 17m 34s\n",
      "212:\tlearn: 0.9141110\ttotal: 4m 44s\tremaining: 17m 32s\n",
      "213:\tlearn: 0.9130112\ttotal: 4m 46s\tremaining: 17m 31s\n",
      "214:\tlearn: 0.9126911\ttotal: 4m 47s\tremaining: 17m 29s\n",
      "215:\tlearn: 0.9122011\ttotal: 4m 48s\tremaining: 17m 28s\n",
      "216:\tlearn: 0.9118317\ttotal: 4m 50s\tremaining: 17m 27s\n",
      "217:\tlearn: 0.9114925\ttotal: 4m 51s\tremaining: 17m 25s\n",
      "218:\tlearn: 0.9109414\ttotal: 4m 52s\tremaining: 17m 24s\n",
      "219:\tlearn: 0.9105865\ttotal: 4m 54s\tremaining: 17m 22s\n",
      "220:\tlearn: 0.9097199\ttotal: 4m 55s\tremaining: 17m 22s\n",
      "221:\tlearn: 0.9094282\ttotal: 4m 57s\tremaining: 17m 20s\n",
      "222:\tlearn: 0.9078017\ttotal: 4m 58s\tremaining: 17m 19s\n",
      "223:\tlearn: 0.9074890\ttotal: 4m 59s\tremaining: 17m 18s\n",
      "224:\tlearn: 0.9064741\ttotal: 5m\tremaining: 17m 16s\n",
      "225:\tlearn: 0.9062339\ttotal: 5m 2s\tremaining: 17m 15s\n",
      "226:\tlearn: 0.9056647\ttotal: 5m 3s\tremaining: 17m 13s\n",
      "227:\tlearn: 0.9054215\ttotal: 5m 4s\tremaining: 17m 12s\n",
      "228:\tlearn: 0.9050837\ttotal: 5m 6s\tremaining: 17m 10s\n",
      "229:\tlearn: 0.9047077\ttotal: 5m 7s\tremaining: 17m 9s\n",
      "230:\tlearn: 0.9042881\ttotal: 5m 8s\tremaining: 17m 7s\n",
      "231:\tlearn: 0.9035353\ttotal: 5m 9s\tremaining: 17m 6s\n",
      "232:\tlearn: 0.9031968\ttotal: 5m 11s\tremaining: 17m 4s\n",
      "233:\tlearn: 0.9028753\ttotal: 5m 12s\tremaining: 17m 2s\n",
      "234:\tlearn: 0.9021211\ttotal: 5m 13s\tremaining: 17m 1s\n",
      "235:\tlearn: 0.9017362\ttotal: 5m 15s\tremaining: 16m 59s\n",
      "236:\tlearn: 0.9011736\ttotal: 5m 16s\tremaining: 16m 58s\n",
      "237:\tlearn: 0.9001232\ttotal: 5m 17s\tremaining: 16m 56s\n",
      "238:\tlearn: 0.8992172\ttotal: 5m 19s\tremaining: 16m 55s\n",
      "239:\tlearn: 0.8988457\ttotal: 5m 20s\tremaining: 16m 54s\n",
      "240:\tlearn: 0.8980314\ttotal: 5m 21s\tremaining: 16m 52s\n",
      "241:\tlearn: 0.8966633\ttotal: 5m 22s\tremaining: 16m 51s\n",
      "242:\tlearn: 0.8963326\ttotal: 5m 24s\tremaining: 16m 49s\n",
      "243:\tlearn: 0.8959270\ttotal: 5m 25s\tremaining: 16m 48s\n",
      "244:\tlearn: 0.8950046\ttotal: 5m 26s\tremaining: 16m 47s\n",
      "245:\tlearn: 0.8944297\ttotal: 5m 28s\tremaining: 16m 45s\n",
      "246:\tlearn: 0.8940342\ttotal: 5m 29s\tremaining: 16m 44s\n",
      "247:\tlearn: 0.8937769\ttotal: 5m 30s\tremaining: 16m 42s\n",
      "248:\tlearn: 0.8931400\ttotal: 5m 31s\tremaining: 16m 41s\n",
      "249:\tlearn: 0.8927436\ttotal: 5m 33s\tremaining: 16m 39s\n",
      "250:\tlearn: 0.8918659\ttotal: 5m 34s\tremaining: 16m 38s\n",
      "251:\tlearn: 0.8914942\ttotal: 5m 35s\tremaining: 16m 37s\n",
      "252:\tlearn: 0.8911910\ttotal: 5m 37s\tremaining: 16m 35s\n",
      "253:\tlearn: 0.8907436\ttotal: 5m 38s\tremaining: 16m 34s\n",
      "254:\tlearn: 0.8897393\ttotal: 5m 39s\tremaining: 16m 32s\n",
      "255:\tlearn: 0.8891768\ttotal: 5m 41s\tremaining: 16m 31s\n",
      "256:\tlearn: 0.8878700\ttotal: 5m 42s\tremaining: 16m 29s\n",
      "257:\tlearn: 0.8866261\ttotal: 5m 43s\tremaining: 16m 28s\n",
      "258:\tlearn: 0.8859743\ttotal: 5m 45s\tremaining: 16m 27s\n",
      "259:\tlearn: 0.8844828\ttotal: 5m 46s\tremaining: 16m 26s\n",
      "260:\tlearn: 0.8841205\ttotal: 5m 47s\tremaining: 16m 24s\n",
      "261:\tlearn: 0.8837381\ttotal: 5m 49s\tremaining: 16m 23s\n",
      "262:\tlearn: 0.8831026\ttotal: 5m 50s\tremaining: 16m 22s\n",
      "263:\tlearn: 0.8827964\ttotal: 5m 51s\tremaining: 16m 20s\n",
      "264:\tlearn: 0.8825586\ttotal: 5m 53s\tremaining: 16m 19s\n",
      "265:\tlearn: 0.8817001\ttotal: 5m 54s\tremaining: 16m 17s\n",
      "266:\tlearn: 0.8811651\ttotal: 5m 55s\tremaining: 16m 16s\n",
      "267:\tlearn: 0.8808967\ttotal: 5m 56s\tremaining: 16m 14s\n",
      "268:\tlearn: 0.8806570\ttotal: 5m 58s\tremaining: 16m 13s\n",
      "269:\tlearn: 0.8798227\ttotal: 5m 59s\tremaining: 16m 12s\n",
      "270:\tlearn: 0.8794832\ttotal: 6m\tremaining: 16m 10s\n",
      "271:\tlearn: 0.8793143\ttotal: 6m 2s\tremaining: 16m 9s\n",
      "272:\tlearn: 0.8786128\ttotal: 6m 3s\tremaining: 16m 7s\n",
      "273:\tlearn: 0.8779614\ttotal: 6m 4s\tremaining: 16m 6s\n",
      "274:\tlearn: 0.8777418\ttotal: 6m 6s\tremaining: 16m 5s\n",
      "275:\tlearn: 0.8769902\ttotal: 6m 7s\tremaining: 16m 3s\n",
      "276:\tlearn: 0.8761067\ttotal: 6m 8s\tremaining: 16m 2s\n",
      "277:\tlearn: 0.8748985\ttotal: 6m 10s\tremaining: 16m 1s\n",
      "278:\tlearn: 0.8744831\ttotal: 6m 11s\tremaining: 16m\n",
      "279:\tlearn: 0.8741088\ttotal: 6m 12s\tremaining: 15m 58s\n",
      "280:\tlearn: 0.8738751\ttotal: 6m 14s\tremaining: 15m 57s\n",
      "281:\tlearn: 0.8733884\ttotal: 6m 15s\tremaining: 15m 55s\n",
      "282:\tlearn: 0.8729846\ttotal: 6m 16s\tremaining: 15m 54s\n",
      "283:\tlearn: 0.8724612\ttotal: 6m 17s\tremaining: 15m 52s\n",
      "284:\tlearn: 0.8720358\ttotal: 6m 19s\tremaining: 15m 51s\n",
      "285:\tlearn: 0.8718856\ttotal: 6m 20s\tremaining: 15m 50s\n",
      "286:\tlearn: 0.8717521\ttotal: 6m 21s\tremaining: 15m 48s\n",
      "287:\tlearn: 0.8715886\ttotal: 6m 23s\tremaining: 15m 47s\n",
      "288:\tlearn: 0.8708687\ttotal: 6m 24s\tremaining: 15m 45s\n",
      "289:\tlearn: 0.8701471\ttotal: 6m 25s\tremaining: 15m 44s\n",
      "290:\tlearn: 0.8699272\ttotal: 6m 27s\tremaining: 15m 43s\n",
      "291:\tlearn: 0.8696973\ttotal: 6m 28s\tremaining: 15m 41s\n",
      "292:\tlearn: 0.8690165\ttotal: 6m 29s\tremaining: 15m 40s\n",
      "293:\tlearn: 0.8681365\ttotal: 6m 31s\tremaining: 15m 39s\n",
      "294:\tlearn: 0.8679093\ttotal: 6m 32s\tremaining: 15m 37s\n",
      "295:\tlearn: 0.8674831\ttotal: 6m 33s\tremaining: 15m 36s\n",
      "296:\tlearn: 0.8670126\ttotal: 6m 35s\tremaining: 15m 35s\n",
      "297:\tlearn: 0.8667244\ttotal: 6m 36s\tremaining: 15m 33s\n",
      "298:\tlearn: 0.8662611\ttotal: 6m 37s\tremaining: 15m 32s\n",
      "299:\tlearn: 0.8659539\ttotal: 6m 38s\tremaining: 15m 30s\n",
      "300:\tlearn: 0.8657067\ttotal: 6m 40s\tremaining: 15m 29s\n",
      "301:\tlearn: 0.8649690\ttotal: 6m 41s\tremaining: 15m 28s\n",
      "302:\tlearn: 0.8648597\ttotal: 6m 42s\tremaining: 15m 27s\n",
      "303:\tlearn: 0.8641814\ttotal: 6m 44s\tremaining: 15m 25s\n",
      "304:\tlearn: 0.8635243\ttotal: 6m 45s\tremaining: 15m 24s\n",
      "305:\tlearn: 0.8631844\ttotal: 6m 46s\tremaining: 15m 22s\n",
      "306:\tlearn: 0.8627957\ttotal: 6m 48s\tremaining: 15m 21s\n",
      "307:\tlearn: 0.8625018\ttotal: 6m 49s\tremaining: 15m 19s\n",
      "308:\tlearn: 0.8622779\ttotal: 6m 50s\tremaining: 15m 18s\n",
      "309:\tlearn: 0.8620087\ttotal: 6m 51s\tremaining: 15m 16s\n",
      "310:\tlearn: 0.8616556\ttotal: 6m 53s\tremaining: 15m 15s\n",
      "311:\tlearn: 0.8613721\ttotal: 6m 54s\tremaining: 15m 14s\n",
      "312:\tlearn: 0.8611306\ttotal: 6m 55s\tremaining: 15m 12s\n",
      "313:\tlearn: 0.8609312\ttotal: 6m 57s\tremaining: 15m 11s\n",
      "314:\tlearn: 0.8605812\ttotal: 6m 58s\tremaining: 15m 9s\n",
      "315:\tlearn: 0.8601972\ttotal: 6m 59s\tremaining: 15m 8s\n",
      "316:\tlearn: 0.8598980\ttotal: 7m\tremaining: 15m 6s\n",
      "317:\tlearn: 0.8596888\ttotal: 7m 2s\tremaining: 15m 5s\n",
      "318:\tlearn: 0.8595265\ttotal: 7m 3s\tremaining: 15m 4s\n",
      "319:\tlearn: 0.8591958\ttotal: 7m 4s\tremaining: 15m 2s\n",
      "320:\tlearn: 0.8584040\ttotal: 7m 6s\tremaining: 15m 1s\n",
      "321:\tlearn: 0.8581966\ttotal: 7m 7s\tremaining: 14m 59s\n",
      "322:\tlearn: 0.8579124\ttotal: 7m 8s\tremaining: 14m 58s\n",
      "323:\tlearn: 0.8577041\ttotal: 7m 10s\tremaining: 14m 57s\n",
      "324:\tlearn: 0.8570602\ttotal: 7m 11s\tremaining: 14m 55s\n",
      "325:\tlearn: 0.8568459\ttotal: 7m 12s\tremaining: 14m 54s\n",
      "326:\tlearn: 0.8567081\ttotal: 7m 13s\tremaining: 14m 53s\n",
      "327:\tlearn: 0.8562387\ttotal: 7m 15s\tremaining: 14m 51s\n",
      "328:\tlearn: 0.8559815\ttotal: 7m 16s\tremaining: 14m 50s\n",
      "329:\tlearn: 0.8558267\ttotal: 7m 17s\tremaining: 14m 48s\n",
      "330:\tlearn: 0.8554401\ttotal: 7m 19s\tremaining: 14m 47s\n",
      "331:\tlearn: 0.8551590\ttotal: 7m 20s\tremaining: 14m 46s\n",
      "332:\tlearn: 0.8544476\ttotal: 7m 21s\tremaining: 14m 44s\n",
      "333:\tlearn: 0.8542768\ttotal: 7m 22s\tremaining: 14m 43s\n",
      "334:\tlearn: 0.8539770\ttotal: 7m 24s\tremaining: 14m 41s\n",
      "335:\tlearn: 0.8538627\ttotal: 7m 25s\tremaining: 14m 40s\n",
      "336:\tlearn: 0.8533085\ttotal: 7m 26s\tremaining: 14m 38s\n",
      "337:\tlearn: 0.8530930\ttotal: 7m 28s\tremaining: 14m 37s\n",
      "338:\tlearn: 0.8528588\ttotal: 7m 29s\tremaining: 14m 36s\n",
      "339:\tlearn: 0.8526622\ttotal: 7m 30s\tremaining: 14m 34s\n",
      "340:\tlearn: 0.8513459\ttotal: 7m 31s\tremaining: 14m 33s\n",
      "341:\tlearn: 0.8510959\ttotal: 7m 33s\tremaining: 14m 31s\n",
      "342:\tlearn: 0.8506085\ttotal: 7m 34s\tremaining: 14m 30s\n",
      "343:\tlearn: 0.8501447\ttotal: 7m 35s\tremaining: 14m 29s\n",
      "344:\tlearn: 0.8499173\ttotal: 7m 37s\tremaining: 14m 27s\n",
      "345:\tlearn: 0.8498001\ttotal: 7m 38s\tremaining: 14m 26s\n",
      "346:\tlearn: 0.8496180\ttotal: 7m 39s\tremaining: 14m 24s\n",
      "347:\tlearn: 0.8494820\ttotal: 7m 40s\tremaining: 14m 23s\n",
      "348:\tlearn: 0.8491744\ttotal: 7m 42s\tremaining: 14m 22s\n",
      "349:\tlearn: 0.8489268\ttotal: 7m 43s\tremaining: 14m 20s\n",
      "350:\tlearn: 0.8485157\ttotal: 7m 44s\tremaining: 14m 19s\n",
      "351:\tlearn: 0.8483065\ttotal: 7m 45s\tremaining: 14m 17s\n",
      "352:\tlearn: 0.8480842\ttotal: 7m 47s\tremaining: 14m 16s\n",
      "353:\tlearn: 0.8477050\ttotal: 7m 48s\tremaining: 14m 15s\n",
      "354:\tlearn: 0.8467716\ttotal: 7m 49s\tremaining: 14m 13s\n",
      "355:\tlearn: 0.8466249\ttotal: 7m 51s\tremaining: 14m 12s\n",
      "356:\tlearn: 0.8464873\ttotal: 7m 52s\tremaining: 14m 10s\n",
      "357:\tlearn: 0.8461949\ttotal: 7m 53s\tremaining: 14m 9s\n",
      "358:\tlearn: 0.8460074\ttotal: 7m 54s\tremaining: 14m 8s\n",
      "359:\tlearn: 0.8458615\ttotal: 7m 56s\tremaining: 14m 6s\n",
      "360:\tlearn: 0.8457065\ttotal: 7m 57s\tremaining: 14m 5s\n",
      "361:\tlearn: 0.8455881\ttotal: 7m 58s\tremaining: 14m 3s\n",
      "362:\tlearn: 0.8448078\ttotal: 8m\tremaining: 14m 2s\n",
      "363:\tlearn: 0.8446466\ttotal: 8m 1s\tremaining: 14m 1s\n",
      "364:\tlearn: 0.8439772\ttotal: 8m 2s\tremaining: 13m 59s\n",
      "365:\tlearn: 0.8437967\ttotal: 8m 3s\tremaining: 13m 58s\n",
      "366:\tlearn: 0.8433355\ttotal: 8m 5s\tremaining: 13m 56s\n",
      "367:\tlearn: 0.8418971\ttotal: 8m 6s\tremaining: 13m 55s\n",
      "368:\tlearn: 0.8416357\ttotal: 8m 7s\tremaining: 13m 54s\n",
      "369:\tlearn: 0.8411622\ttotal: 8m 9s\tremaining: 13m 53s\n",
      "370:\tlearn: 0.8409931\ttotal: 8m 10s\tremaining: 13m 51s\n",
      "371:\tlearn: 0.8408151\ttotal: 8m 11s\tremaining: 13m 50s\n",
      "372:\tlearn: 0.8404537\ttotal: 8m 13s\tremaining: 13m 48s\n",
      "373:\tlearn: 0.8402989\ttotal: 8m 14s\tremaining: 13m 47s\n",
      "374:\tlearn: 0.8399947\ttotal: 8m 15s\tremaining: 13m 46s\n",
      "375:\tlearn: 0.8394636\ttotal: 8m 16s\tremaining: 13m 44s\n",
      "376:\tlearn: 0.8393344\ttotal: 8m 18s\tremaining: 13m 43s\n",
      "377:\tlearn: 0.8390961\ttotal: 8m 19s\tremaining: 13m 42s\n",
      "378:\tlearn: 0.8385288\ttotal: 8m 21s\tremaining: 13m 41s\n",
      "379:\tlearn: 0.8380904\ttotal: 8m 22s\tremaining: 13m 39s\n",
      "380:\tlearn: 0.8377048\ttotal: 8m 23s\tremaining: 13m 38s\n",
      "381:\tlearn: 0.8375633\ttotal: 8m 24s\tremaining: 13m 36s\n",
      "382:\tlearn: 0.8372271\ttotal: 8m 26s\tremaining: 13m 35s\n",
      "383:\tlearn: 0.8367905\ttotal: 8m 27s\tremaining: 13m 34s\n",
      "384:\tlearn: 0.8364833\ttotal: 8m 28s\tremaining: 13m 32s\n",
      "385:\tlearn: 0.8356713\ttotal: 8m 30s\tremaining: 13m 31s\n",
      "386:\tlearn: 0.8355501\ttotal: 8m 31s\tremaining: 13m 30s\n",
      "387:\tlearn: 0.8348945\ttotal: 8m 32s\tremaining: 13m 28s\n",
      "388:\tlearn: 0.8346255\ttotal: 8m 34s\tremaining: 13m 27s\n",
      "389:\tlearn: 0.8342820\ttotal: 8m 35s\tremaining: 13m 26s\n",
      "390:\tlearn: 0.8340366\ttotal: 8m 36s\tremaining: 13m 25s\n",
      "391:\tlearn: 0.8335867\ttotal: 8m 38s\tremaining: 13m 23s\n",
      "392:\tlearn: 0.8333149\ttotal: 8m 39s\tremaining: 13m 22s\n",
      "393:\tlearn: 0.8331044\ttotal: 8m 40s\tremaining: 13m 20s\n",
      "394:\tlearn: 0.8328768\ttotal: 8m 41s\tremaining: 13m 19s\n",
      "395:\tlearn: 0.8323610\ttotal: 8m 43s\tremaining: 13m 18s\n",
      "396:\tlearn: 0.8321462\ttotal: 8m 44s\tremaining: 13m 16s\n",
      "397:\tlearn: 0.8314917\ttotal: 8m 45s\tremaining: 13m 15s\n",
      "398:\tlearn: 0.8312357\ttotal: 8m 47s\tremaining: 13m 13s\n",
      "399:\tlearn: 0.8307556\ttotal: 8m 48s\tremaining: 13m 12s\n",
      "400:\tlearn: 0.8305245\ttotal: 8m 49s\tremaining: 13m 11s\n",
      "401:\tlearn: 0.8302478\ttotal: 8m 50s\tremaining: 13m 9s\n",
      "402:\tlearn: 0.8300584\ttotal: 8m 52s\tremaining: 13m 8s\n",
      "403:\tlearn: 0.8298499\ttotal: 8m 53s\tremaining: 13m 7s\n",
      "404:\tlearn: 0.8296500\ttotal: 8m 54s\tremaining: 13m 5s\n",
      "405:\tlearn: 0.8294812\ttotal: 8m 56s\tremaining: 13m 4s\n",
      "406:\tlearn: 0.8292829\ttotal: 8m 57s\tremaining: 13m 2s\n",
      "407:\tlearn: 0.8283316\ttotal: 8m 58s\tremaining: 13m 1s\n",
      "408:\tlearn: 0.8280454\ttotal: 9m\tremaining: 13m\n",
      "409:\tlearn: 0.8279450\ttotal: 9m 1s\tremaining: 12m 59s\n",
      "410:\tlearn: 0.8277730\ttotal: 9m 2s\tremaining: 12m 57s\n",
      "411:\tlearn: 0.8270930\ttotal: 9m 3s\tremaining: 12m 56s\n",
      "412:\tlearn: 0.8268993\ttotal: 9m 5s\tremaining: 12m 54s\n",
      "413:\tlearn: 0.8267734\ttotal: 9m 6s\tremaining: 12m 53s\n",
      "414:\tlearn: 0.8265532\ttotal: 9m 7s\tremaining: 12m 52s\n",
      "415:\tlearn: 0.8263112\ttotal: 9m 9s\tremaining: 12m 50s\n",
      "416:\tlearn: 0.8261710\ttotal: 9m 10s\tremaining: 12m 49s\n",
      "417:\tlearn: 0.8257571\ttotal: 9m 11s\tremaining: 12m 48s\n",
      "418:\tlearn: 0.8255154\ttotal: 9m 12s\tremaining: 12m 46s\n",
      "419:\tlearn: 0.8248131\ttotal: 9m 14s\tremaining: 12m 45s\n",
      "420:\tlearn: 0.8246730\ttotal: 9m 15s\tremaining: 12m 43s\n",
      "421:\tlearn: 0.8237529\ttotal: 9m 16s\tremaining: 12m 42s\n",
      "422:\tlearn: 0.8232928\ttotal: 9m 18s\tremaining: 12m 41s\n",
      "423:\tlearn: 0.8228317\ttotal: 9m 19s\tremaining: 12m 39s\n",
      "424:\tlearn: 0.8224557\ttotal: 9m 20s\tremaining: 12m 38s\n",
      "425:\tlearn: 0.8223435\ttotal: 9m 21s\tremaining: 12m 37s\n",
      "426:\tlearn: 0.8221804\ttotal: 9m 23s\tremaining: 12m 35s\n",
      "427:\tlearn: 0.8219562\ttotal: 9m 24s\tremaining: 12m 34s\n",
      "428:\tlearn: 0.8210972\ttotal: 9m 25s\tremaining: 12m 33s\n",
      "429:\tlearn: 0.8209510\ttotal: 9m 27s\tremaining: 12m 31s\n",
      "430:\tlearn: 0.8203209\ttotal: 9m 28s\tremaining: 12m 30s\n",
      "431:\tlearn: 0.8199971\ttotal: 9m 29s\tremaining: 12m 28s\n",
      "432:\tlearn: 0.8196018\ttotal: 9m 30s\tremaining: 12m 27s\n",
      "433:\tlearn: 0.8194649\ttotal: 9m 32s\tremaining: 12m 26s\n",
      "434:\tlearn: 0.8185203\ttotal: 9m 33s\tremaining: 12m 24s\n",
      "435:\tlearn: 0.8182142\ttotal: 9m 34s\tremaining: 12m 23s\n",
      "436:\tlearn: 0.8177001\ttotal: 9m 36s\tremaining: 12m 22s\n",
      "437:\tlearn: 0.8175986\ttotal: 9m 37s\tremaining: 12m 20s\n",
      "438:\tlearn: 0.8170995\ttotal: 9m 38s\tremaining: 12m 19s\n",
      "439:\tlearn: 0.8168984\ttotal: 9m 39s\tremaining: 12m 18s\n",
      "440:\tlearn: 0.8167219\ttotal: 9m 41s\tremaining: 12m 16s\n",
      "441:\tlearn: 0.8165710\ttotal: 9m 42s\tremaining: 12m 15s\n",
      "442:\tlearn: 0.8163374\ttotal: 9m 43s\tremaining: 12m 14s\n",
      "443:\tlearn: 0.8159415\ttotal: 9m 45s\tremaining: 12m 12s\n",
      "444:\tlearn: 0.8155725\ttotal: 9m 46s\tremaining: 12m 11s\n",
      "445:\tlearn: 0.8152348\ttotal: 9m 47s\tremaining: 12m 9s\n",
      "446:\tlearn: 0.8151207\ttotal: 9m 48s\tremaining: 12m 8s\n",
      "447:\tlearn: 0.8150249\ttotal: 9m 50s\tremaining: 12m 7s\n",
      "448:\tlearn: 0.8148960\ttotal: 9m 51s\tremaining: 12m 5s\n",
      "449:\tlearn: 0.8146983\ttotal: 9m 52s\tremaining: 12m 4s\n",
      "450:\tlearn: 0.8141401\ttotal: 9m 54s\tremaining: 12m 3s\n",
      "451:\tlearn: 0.8134644\ttotal: 9m 55s\tremaining: 12m 1s\n",
      "452:\tlearn: 0.8133787\ttotal: 9m 56s\tremaining: 12m\n",
      "453:\tlearn: 0.8131656\ttotal: 9m 57s\tremaining: 11m 59s\n",
      "454:\tlearn: 0.8130176\ttotal: 9m 59s\tremaining: 11m 57s\n",
      "455:\tlearn: 0.8128469\ttotal: 10m\tremaining: 11m 56s\n",
      "456:\tlearn: 0.8126993\ttotal: 10m 1s\tremaining: 11m 55s\n",
      "457:\tlearn: 0.8122103\ttotal: 10m 3s\tremaining: 11m 53s\n",
      "458:\tlearn: 0.8120140\ttotal: 10m 4s\tremaining: 11m 52s\n",
      "459:\tlearn: 0.8117186\ttotal: 10m 5s\tremaining: 11m 50s\n",
      "460:\tlearn: 0.8115961\ttotal: 10m 6s\tremaining: 11m 49s\n",
      "461:\tlearn: 0.8113874\ttotal: 10m 8s\tremaining: 11m 48s\n",
      "462:\tlearn: 0.8113123\ttotal: 10m 9s\tremaining: 11m 46s\n",
      "463:\tlearn: 0.8111332\ttotal: 10m 10s\tremaining: 11m 45s\n",
      "464:\tlearn: 0.8109505\ttotal: 10m 12s\tremaining: 11m 44s\n",
      "465:\tlearn: 0.8107228\ttotal: 10m 13s\tremaining: 11m 42s\n",
      "466:\tlearn: 0.8105957\ttotal: 10m 14s\tremaining: 11m 41s\n",
      "467:\tlearn: 0.8103450\ttotal: 10m 15s\tremaining: 11m 40s\n",
      "468:\tlearn: 0.8100691\ttotal: 10m 17s\tremaining: 11m 38s\n",
      "469:\tlearn: 0.8099568\ttotal: 10m 18s\tremaining: 11m 37s\n",
      "470:\tlearn: 0.8096846\ttotal: 10m 19s\tremaining: 11m 35s\n",
      "471:\tlearn: 0.8095710\ttotal: 10m 20s\tremaining: 11m 34s\n",
      "472:\tlearn: 0.8094400\ttotal: 10m 22s\tremaining: 11m 33s\n",
      "473:\tlearn: 0.8092344\ttotal: 10m 23s\tremaining: 11m 31s\n",
      "474:\tlearn: 0.8090108\ttotal: 10m 24s\tremaining: 11m 30s\n",
      "475:\tlearn: 0.8089256\ttotal: 10m 26s\tremaining: 11m 29s\n",
      "476:\tlearn: 0.8086119\ttotal: 10m 27s\tremaining: 11m 27s\n",
      "477:\tlearn: 0.8084648\ttotal: 10m 28s\tremaining: 11m 26s\n",
      "478:\tlearn: 0.8077157\ttotal: 10m 30s\tremaining: 11m 25s\n",
      "479:\tlearn: 0.8074986\ttotal: 10m 31s\tremaining: 11m 24s\n",
      "480:\tlearn: 0.8073374\ttotal: 10m 32s\tremaining: 11m 22s\n",
      "481:\tlearn: 0.8072152\ttotal: 10m 34s\tremaining: 11m 21s\n",
      "482:\tlearn: 0.8071070\ttotal: 10m 35s\tremaining: 11m 20s\n",
      "483:\tlearn: 0.8058582\ttotal: 10m 36s\tremaining: 11m 18s\n",
      "484:\tlearn: 0.8055057\ttotal: 10m 38s\tremaining: 11m 17s\n",
      "485:\tlearn: 0.8054573\ttotal: 10m 39s\tremaining: 11m 16s\n",
      "486:\tlearn: 0.8053835\ttotal: 10m 40s\tremaining: 11m 14s\n",
      "487:\tlearn: 0.8052960\ttotal: 10m 41s\tremaining: 11m 13s\n",
      "488:\tlearn: 0.8051675\ttotal: 10m 43s\tremaining: 11m 12s\n",
      "489:\tlearn: 0.8050235\ttotal: 10m 44s\tremaining: 11m 10s\n",
      "490:\tlearn: 0.8044242\ttotal: 10m 45s\tremaining: 11m 9s\n",
      "491:\tlearn: 0.8042800\ttotal: 10m 47s\tremaining: 11m 8s\n",
      "492:\tlearn: 0.8036875\ttotal: 10m 48s\tremaining: 11m 6s\n",
      "493:\tlearn: 0.8033428\ttotal: 10m 49s\tremaining: 11m 5s\n",
      "494:\tlearn: 0.8030874\ttotal: 10m 51s\tremaining: 11m 4s\n",
      "495:\tlearn: 0.8025173\ttotal: 10m 52s\tremaining: 11m 3s\n",
      "496:\tlearn: 0.8020998\ttotal: 10m 54s\tremaining: 11m 1s\n"
     ]
    }
   ],
   "source": [
    "#Catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "X = data1.copy()\n",
    "X.drop(columns=['genre', 'subgenre'], inplace=True)\n",
    "\n",
    "y = data1['genre']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "cv = TfidfVectorizer()\n",
    "\n",
    "X_train['text'] = X_train['text'].apply(' '.join)\n",
    "X_test['text'] = X_test['text'].apply(' '.join)\n",
    "\n",
    "X_train_text = cv.fit_transform(X_train['text'])\n",
    "X_test_text = cv.transform(X_test['text'])\n",
    "\n",
    "X_train.drop(columns='text', inplace=True)\n",
    "X_test.drop(columns='text', inplace=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num = scaler.fit_transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "\n",
    "X_train = hstack([X_train_text, X_train_num])\n",
    "X_test = hstack([X_test_text, X_test_num])\n",
    "\n",
    "catboost = CatBoostClassifier()\n",
    "\n",
    "catboost.fit(X_train, y_train)\n",
    "\n",
    "y_pred = catboost.predict(X_test)\n",
    "\n",
    "classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         edm       0.74      0.81      0.77       747\n",
      "       latin       0.80      0.74      0.77       748\n",
      "         pop       0.53      0.44      0.48       748\n",
      "         r&b       0.64      0.68      0.66       748\n",
      "         rap       0.80      0.79      0.79       748\n",
      "        rock       0.71      0.79      0.75       748\n",
      "\n",
      "    accuracy                           0.71      4487\n",
      "   macro avg       0.71      0.71      0.71      4487\n",
      "weighted avg       0.71      0.71      0.71      4487\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
